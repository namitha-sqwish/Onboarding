{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f48ff737",
   "metadata": {},
   "source": [
    "## Design: \n",
    "\n",
    "Propose a logging schema for Sqwish’s online learning system. It should record everything needed to reproduce training and do later analysis. For each user request, what would you log? (Think: a unique request ID, user context features, chosen prompt/model, all model outputs maybe, any user click or outcome, timestamps, the probability or propensity of the chosen action if using a stochastic policy, etc.). Write a structured list of fields and justify each (why is it needed? e.g. propensity is needed for IPS in OPE).\n",
    "\n",
    "## Thoughts : \n",
    "\n",
    "request_id\n",
    "\n",
    "timestamp\n",
    "\n",
    "user context features - Needed to train the Reward Model ($q$) for Doubly Robust estimation.\n",
    "\n",
    "action taken (choosen model or prompt) - IPS calculation\n",
    "\n",
    "reward (click or outcome) - needed for reward estimate model and IPS \n",
    "\n",
    "propensity - needed for IPS calculation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6024f5",
   "metadata": {},
   "source": [
    "## Analysis: \n",
    "You have deployed a bandit that optimizes prompts. After a week, you examine results and see overall user satisfaction went up 5%. However, for new users (first-time visitors) satisfaction dropped. How would you investigate this? Outline an experiment or analysis using logs to diagnose why the policy might be underperforming for new users (maybe it over-explored or didn’t personalize properly for cold-start users). What changes to the algorithm could you consider (e.g. epsilon-greedy for new users until enough data)?\n",
    "\n",
    "## Solution : \n",
    "\n",
    "Based on the user features or capture separately if a user is new and analyse for new users subgroup. \n",
    "\n",
    "Hypothesis 1: Cold Start/Exploration. If the agent is using a \"Greedy\" policy based on older users, it might be showing new users content that requires historical context they don't have.\n",
    "\n",
    "Hypothesis 2: Feature Shift. Perhaps the features used for \"New Users\" are sparse (all zeros), leading the model to default to a \"global average\" action that is offensive or irrelevant to a first-time visitor.\n",
    "\n",
    "Algorithmic Changes:\n",
    "\n",
    "Epsilon-Greedy Reset: For new users, force a higher $\\epsilon$ (exploration) to learn their specific preferences faster.\n",
    "\n",
    "Contextual Fallback: If user_history is empty, use a specialized \"Onboarding Policy\" rather than the general optimization policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900e10b3",
   "metadata": {},
   "source": [
    "## Coding: \n",
    "Using an open bandit dataset (e.g. the Open Bandit Pipeline’s logged data if available, or simulate one), perform an off-policy evaluation of a hypothetical new policy. For example, use logged data from a uniform random policy on a classification task as bandit feedback. Define a new deterministic policy (like always choose arm 1 for certain feature values and arm 2 otherwise). Use IPS and Doubly Robust to estimate the new policy’s reward from the logs. Compare that to the actual reward if you run the new policy on the dataset (if ground truth available). This exercise solidifies understanding of OPE’s value and limitations (if the policy is very different, IPS variance will be high)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d347cf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/namitha/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_linear_loss.py:200: RuntimeWarning: divide by zero encountered in matmul\n",
      "  raw_prediction = X @ weights + intercept\n",
      "/Users/namitha/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_linear_loss.py:200: RuntimeWarning: overflow encountered in matmul\n",
      "  raw_prediction = X @ weights + intercept\n",
      "/Users/namitha/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_linear_loss.py:200: RuntimeWarning: invalid value encountered in matmul\n",
      "  raw_prediction = X @ weights + intercept\n",
      "/Users/namitha/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_linear_loss.py:200: RuntimeWarning: divide by zero encountered in matmul\n",
      "  raw_prediction = X @ weights + intercept\n",
      "/Users/namitha/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_linear_loss.py:200: RuntimeWarning: overflow encountered in matmul\n",
      "  raw_prediction = X @ weights + intercept\n",
      "/Users/namitha/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_linear_loss.py:200: RuntimeWarning: invalid value encountered in matmul\n",
      "  raw_prediction = X @ weights + intercept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth Value:  0.6503\n",
      "IPS Estimate:        0.6580 (Error: 0.0077)\n",
      "DR Estimate:         0.6572 (Error: 0.0069)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 1. Setup Simulation Parameters\n",
    "np.random.seed(42)\n",
    "n_samples = 2000\n",
    "n_features = 2\n",
    "\n",
    "# Generate random contexts (x)\n",
    "X = np.random.uniform(0, 1, size=(n_samples, n_features))\n",
    "\n",
    "# Define 'Ground Truth' reward probability (Unknown to the agent)\n",
    "# Action 0 likes Feature 1; Action 1 likes Feature 0\n",
    "def get_true_reward_prob(context, action):\n",
    "    if action == 0:\n",
    "        return 0.8 if context[1] > 0.5 else 0.2\n",
    "    else:\n",
    "        return 0.8 if context[0] > 0.5 else 0.2\n",
    "\n",
    "# 2. LOGGING DATA: Uniform Random Policy (Behavior Policy)\n",
    "actions_logged = []\n",
    "rewards_logged = []\n",
    "propensities_logged = []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    # Logging Policy: p(a=0) = 0.5, p(a=1) = 0.5\n",
    "    action = np.random.choice([0, 1])\n",
    "    prob = 0.5\n",
    "    \n",
    "    # Observe Reward\n",
    "    r_prob = get_true_reward_prob(X[i], action)\n",
    "    reward = np.random.binomial(1, r_prob)\n",
    "    \n",
    "    actions_logged.append(action)\n",
    "    rewards_logged.append(reward)\n",
    "    propensities_logged.append(prob)\n",
    "\n",
    "# 3. DEFINE TARGET POLICY (Hypothetical New Policy)\n",
    "# Logic: Use Action 1 if Feature 0 > 0.5, else Action 0\n",
    "def target_policy(context):\n",
    "    return 1 if context[0] > 0.5 else 0\n",
    "\n",
    "# 4. GROUND TRUTH CALCULATION (What would actually happen?)\n",
    "true_rewards = [get_true_reward_prob(X[i], target_policy(X[i])) for i in range(n_samples)]\n",
    "ground_truth_val = np.mean(true_rewards)\n",
    "\n",
    "# 5. OPE ESTIMATION: IPS\n",
    "ips_scores = []\n",
    "for i in range(n_samples):\n",
    "    pi_a = 1.0 if actions_logged[i] == target_policy(X[i]) else 0.0\n",
    "    ips_scores.append(rewards_logged[i] * (pi_a / propensities_logged[i]))\n",
    "ips_val = np.mean(ips_scores)\n",
    "\n",
    "# 6. OPE ESTIMATION: Doubly Robust (DR)\n",
    "# Step A: Train a Reward Model (Direct Method)\n",
    "# We train a model to predict reward given (context, action)\n",
    "q_models = {}\n",
    "for a in [0, 1]:\n",
    "    mask = np.array(actions_logged) == a\n",
    "    model = LogisticRegression().fit(X[mask], np.array(rewards_logged)[mask])\n",
    "    q_models[a] = model\n",
    "\n",
    "# Step B: Calculate DR\n",
    "dr_scores = []\n",
    "for i in range(n_samples):\n",
    "    target_a = target_policy(X[i])\n",
    "    actual_a = actions_logged[i]\n",
    "    pi_a = 1.0 if actual_a == target_a else 0.0\n",
    "    \n",
    "    # Model predictions\n",
    "    q_target = q_models[target_a].predict_proba(X[i:i+1])[0][1]\n",
    "    q_actual = q_models[actual_a].predict_proba(X[i:i+1])[0][1]\n",
    "    \n",
    "    # DR Formula: q_target + (indicator/propensity) * (reward - q_actual)\n",
    "    dr_val = q_target + (pi_a / propensities_logged[i]) * (rewards_logged[i] - q_actual)\n",
    "    dr_scores.append(dr_val)\n",
    "dr_val = np.mean(dr_scores)\n",
    "\n",
    "# --- FINAL RESULTS ---\n",
    "print(f\"Ground Truth Value:  {ground_truth_val:.4f}\")\n",
    "print(f\"IPS Estimate:        {ips_val:.4f} (Error: {abs(ips_val-ground_truth_val):.4f})\")\n",
    "print(f\"DR Estimate:         {dr_val:.4f} (Error: {abs(dr_val-ground_truth_val):.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
