{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76bf5f6b",
   "metadata": {},
   "source": [
    "Understanding: Recreate the RLHF loop in your own words: first supervised fine-tune (SFT) a model, then train a reward model on comparisons, then run PPO. Why is the KL divergence term (keeping $\\pi_\\theta$ close to a reference model $\\pi_{\\text{ref}}$) crucial? What happens if $\\beta$ (the KL penalty coefficient) is set too low or zero?\n",
    "\n",
    "1. RLHF loop => pre training -> SFT -> train a reward model which captures how human ranks various outputs(comparision) -> then run a PPO on policy(LM) to give outputs which maximize the reward as per reward model along with outputs which makes sense i.e, are closer to data used in pre training. \n",
    "\n",
    "2. KL - Divergence : KL divergance term is used to penalise the reward given by the reward model. Penalized by the KL divergance between difference in log probabilities of output tokens by Policy(LM) and frozen model after SFT. This ensures that there is no reward hacking i.e, our policy doesn't just output anything to maximize the reward but also ensures that the answer is close to what would have been outputted by SFT model. If the KL penalty coefficient is set too low or zero, the effect of this penality is too low so there is a possibility that policy learns to reward hack.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508bcebc",
   "metadata": {},
   "source": [
    "Now to calculate advantages using generalised advantage estimator, given a sequence of values and rewards, we need to calculate the advantage\n",
    "\n",
    "#delta_t = r_t + gamma * V(s_t+1) - V(s_t)\n",
    "\n",
    "#advantage_t = delta_t + (gamma * lambda * advantage_t+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92c416ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GAE(rewards, values, gamma, lamda):\n",
    "    advantages = []\n",
    "    # default value for t+1 = 0\n",
    "    advantage = 0\n",
    "    # [t-1,0] \n",
    "    for i in range(len(rewards) - 1, -1, -1):\n",
    "        delta = rewards[i] + gamma * (values[i+1] if i+1 < len(values) else 0) - values[i]\n",
    "        advantage = delta + gamma * lamda * advantage\n",
    "        advantages.append(advantage)\n",
    "    return advantages[::-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6c35c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case:\n",
      "Rewards: [1.0, 2.0, 3.0, 4.0]\n",
      "Values: [10.0, 11.0, 12.0, 13.0]\n",
      "Gamma: 0.9, Lambda: 0.95\n",
      "Terminal value: 0.0\n",
      "\n",
      "Calculated Advantages: ['-1.212470', '-2.470725', '-4.995000', '-9.000000']\n",
      "\n",
      "Manual Verification (backwards computation):\n",
      "------------------------------------------------------------\n",
      "t=3:\n",
      "  delta_3 = r_3 + γ * V(s_4) - V(s_3)\n",
      "  delta_3 = 4.0 + 0.9 * 0.0 - 13.0\n",
      "  delta_3 = -9.000000\n",
      "  A_3 = delta_3 + (γ * λ) * A_4\n",
      "  A_3 = -9.000000 + (0.9 * 0.95) * 0.000000\n",
      "  A_3 = -9.000000\n",
      "\n",
      "t=2:\n",
      "  delta_2 = r_2 + γ * V(s_3) - V(s_2)\n",
      "  delta_2 = 3.0 + 0.9 * 13.0 - 12.0\n",
      "  delta_2 = 2.700000\n",
      "  A_2 = delta_2 + (γ * λ) * A_3\n",
      "  A_2 = 2.700000 + (0.9 * 0.95) * -9.000000\n",
      "  A_2 = -4.995000\n",
      "\n",
      "t=1:\n",
      "  delta_1 = r_1 + γ * V(s_2) - V(s_1)\n",
      "  delta_1 = 2.0 + 0.9 * 12.0 - 11.0\n",
      "  delta_1 = 1.800000\n",
      "  A_1 = delta_1 + (γ * λ) * A_2\n",
      "  A_1 = 1.800000 + (0.9 * 0.95) * -4.995000\n",
      "  A_1 = -2.470725\n",
      "\n",
      "t=0:\n",
      "  delta_0 = r_0 + γ * V(s_1) - V(s_0)\n",
      "  delta_0 = 1.0 + 0.9 * 11.0 - 10.0\n",
      "  delta_0 = 0.900000\n",
      "  A_0 = delta_0 + (γ * λ) * A_1\n",
      "  A_0 = 0.900000 + (0.9 * 0.95) * -2.470725\n",
      "  A_0 = -1.212470\n",
      "\n",
      "Comparison:\n",
      "Implementation: ['-1.212470', '-2.470725', '-4.995000', '-9.000000']\n",
      "Manual calc:    ['-1.212470', '-2.470725', '-4.995000', '-9.000000']\n",
      "Match: True\n"
     ]
    }
   ],
   "source": [
    "# Verification: Test GAE implementation with synthetic sequence\n",
    "import numpy as np\n",
    "\n",
    "# Test case: 4 timesteps\n",
    "rewards = [1.0, 2.0, 3.0, 4.0]\n",
    "values = [10.0, 11.0, 12.0, 13.0]  # V(s_t) for each state\n",
    "gamma = 0.9\n",
    "lamda = 0.95\n",
    "terminal_value = 0.0  # V(s_4) = 0 for terminal state (assumed by function)\n",
    "\n",
    "# Calculate advantages using our implementation\n",
    "advantages = GAE(rewards, values, gamma, lamda)\n",
    "\n",
    "print(\"Test Case:\")\n",
    "print(f\"Rewards: {rewards}\")\n",
    "print(f\"Values: {values}\")\n",
    "print(f\"Gamma: {gamma}, Lambda: {lamda}\")\n",
    "print(f\"Terminal value: {terminal_value}\")\n",
    "print(f\"\\nCalculated Advantages: {[f'{a:.6f}' for a in advantages]}\")\n",
    "print()\n",
    "\n",
    "# Manual calculation for verification\n",
    "# We'll calculate backwards from t=3 to t=0\n",
    "print(\"Manual Verification (backwards computation):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Initialize\n",
    "manual_advantages = [0] * len(rewards)\n",
    "advantage_next = 0  # A_4 = 0 (terminal state)\n",
    "\n",
    "# Calculate backwards (from t=3 to t=0)\n",
    "for t in range(len(rewards) - 1, -1, -1):\n",
    "    # Get next state value\n",
    "    if t + 1 < len(values):\n",
    "        next_val = values[t + 1]\n",
    "    else:\n",
    "        next_val = terminal_value\n",
    "    \n",
    "    # Calculate TD error: delta_t = r_t + gamma * V(s_{t+1}) - V(s_t)\n",
    "    delta = rewards[t] + gamma * next_val - values[t]\n",
    "    \n",
    "    # Calculate advantage: A_t = delta_t + (gamma * lambda) * A_{t+1}\n",
    "    # advantage_next holds A_{t+1} from previous iteration\n",
    "    A_t_plus_1 = advantage_next  # Store before updating\n",
    "    advantage = delta + gamma * lamda * advantage_next\n",
    "    manual_advantages[t] = advantage\n",
    "    advantage_next = advantage  # Update for next iteration\n",
    "    \n",
    "    print(f\"t={t}:\")\n",
    "    print(f\"  delta_{t} = r_{t} + γ * V(s_{t+1}) - V(s_{t})\")\n",
    "    print(f\"  delta_{t} = {rewards[t]} + {gamma} * {next_val} - {values[t]}\")\n",
    "    print(f\"  delta_{t} = {delta:.6f}\")\n",
    "    print(f\"  A_{t} = delta_{t} + (γ * λ) * A_{t+1}\")\n",
    "    print(f\"  A_{t} = {delta:.6f} + ({gamma} * {lamda}) * {A_t_plus_1:.6f}\")\n",
    "    print(f\"  A_{t} = {advantage:.6f}\")\n",
    "    print()\n",
    "\n",
    "print(\"Comparison:\")\n",
    "print(f\"Implementation: {[f'{a:.6f}' for a in advantages]}\")\n",
    "print(f\"Manual calc:    {[f'{a:.6f}' for a in manual_advantages]}\")\n",
    "print(f\"Match: {np.allclose(advantages, manual_advantages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "315dd6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Additional Test: Simple case\n",
      "============================================================\n",
      "Rewards: [1.0, 2.0]\n",
      "Values: [0.0, 1.0]\n",
      "Gamma: 0.5, Lambda: 0.0\n",
      "\n",
      "Advantages (should equal TD errors when lambda=0): [1.5, 1.0]\n",
      "Expected (TD errors): [1.5, 1.0]\n",
      "Match: True\n",
      "\n",
      "============================================================\n",
      "Test: Lambda=1 (Monte Carlo)\n",
      "============================================================\n",
      "Rewards: [1.0, 2.0, 3.0]\n",
      "Values: [0.0, 0.0, 0.0]\n",
      "Gamma: 1.0, Lambda: 1.0\n",
      "\n",
      "Advantages: [6.0, 5.0, 3.0]\n",
      "Expected: [6.0, 5.0, 3.0]\n",
      "Match: True\n"
     ]
    }
   ],
   "source": [
    "# Additional test: Simple case with known answer\n",
    "print(\"=\" * 60)\n",
    "print(\"Additional Test: Simple case\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simple test case: lambda=0 (should give TD errors)\n",
    "rewards_simple = [1.0, 2.0]\n",
    "values_simple = [0.0, 1.0]\n",
    "gamma_simple = 0.5\n",
    "lamda_simple = 0.0  # When lambda=0, GAE reduces to TD error\n",
    "\n",
    "advantages_simple = GAE(rewards_simple, values_simple, gamma_simple, lamda_simple)\n",
    "\n",
    "print(f\"Rewards: {rewards_simple}\")\n",
    "print(f\"Values: {values_simple}\")\n",
    "print(f\"Gamma: {gamma_simple}, Lambda: {lamda_simple}\")\n",
    "print(f\"\\nAdvantages (should equal TD errors when lambda=0): {advantages_simple}\")\n",
    "\n",
    "# Expected: A_0 = r_0 + gamma*V(s_1) - V(s_0) = 1 + 0.5*1 - 0 = 1.5\n",
    "#           A_1 = r_1 + gamma*V(s_2) - V(s_1) = 2 + 0.5*0 - 1 = 1.0\n",
    "expected_simple = [1.5, 1.0]\n",
    "print(f\"Expected (TD errors): {expected_simple}\")\n",
    "print(f\"Match: {np.allclose(advantages_simple, expected_simple)}\")\n",
    "print()\n",
    "\n",
    "# Test case: lambda=1 (monte carlo return)\n",
    "print(\"=\" * 60)\n",
    "print(\"Test: Lambda=1 (Monte Carlo)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "rewards_mc = [1.0, 2.0, 3.0]\n",
    "values_mc = [0.0, 0.0, 0.0]\n",
    "gamma_mc = 1.0\n",
    "lamda_mc = 1.0  # When lambda=1, GAE gives Monte Carlo advantages\n",
    "\n",
    "advantages_mc = GAE(rewards_mc, values_mc, gamma_mc, lamda_mc)\n",
    "print(f\"Rewards: {rewards_mc}\")\n",
    "print(f\"Values: {values_mc}\")\n",
    "print(f\"Gamma: {gamma_mc}, Lambda: {lamda_mc}\")\n",
    "print(f\"\\nAdvantages: {advantages_mc}\")\n",
    "\n",
    "# Expected with lambda=1 and gamma=1:\n",
    "# A_2 = r_2 + V(s_3) - V(s_2) = 3 + 0 - 0 = 3\n",
    "# A_1 = r_1 + V(s_2) - V(s_1) + gamma*lambda*A_2 = 2 + 0 - 0 + 1*1*3 = 5\n",
    "# A_0 = r_0 + V(s_1) - V(s_0) + gamma*lambda*A_1 = 1 + 0 - 0 + 1*1*5 = 6\n",
    "# Actually, let's recalculate more carefully:\n",
    "# A_2 = delta_2 = 3 + 1*0 - 0 = 3\n",
    "# A_1 = delta_1 + gamma*lambda*A_2 = (2 + 1*0 - 0) + 1*1*3 = 2 + 3 = 5  \n",
    "# A_0 = delta_0 + gamma*lambda*A_1 = (1 + 1*0 - 0) + 1*1*5 = 1 + 5 = 6\n",
    "expected_mc = [6.0, 5.0, 3.0]\n",
    "print(f\"Expected: {expected_mc}\")\n",
    "print(f\"Match: {np.allclose(advantages_mc, expected_mc)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
