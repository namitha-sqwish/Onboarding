{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Coding\n",
    "**Simulate a FrugalGPT cascade. Assume you have two models: A (cheap, 80% accuracy) and B (expensive, 90% accuracy). Define a simple confidence heuristic for model A (e.g. length of answer or presence of a certain keyword). Implement a policy that calls A, checks confidence; if confident, use A’s answer, if not, call B. Generate a dataset of queries with “ground truth” answers and simulate the cascade, measuring overall accuracy and cost. Compare this to always using B and always using A. Show how varying the confidence threshold produces a Pareto curve of cost vs. accuracy.**\n",
    "\n",
    "frugalgpt -> if cheaper model crosses the confidence threshold then exit and use it's answer else sequentially call other models and do the same. \n",
    "\n",
    "2 open source models, confidence heuristic based on length of the answer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fe2385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FrugalGPT Cascade Logic\n",
    "def cascade_policy(model_a, model_b, query, threshold):\n",
    "    # Call cheap Model A\n",
    "    answer_a, length_a = model_a.generate(query)\n",
    "    \n",
    "    # Calculate Confidence - length based heuristic\n",
    "    confidence = min(1.0, length_a / 50.0) if length_a <= 50 else 1.0\n",
    "    \n",
    "    if confidence >= threshold:\n",
    "        return answer_a, \"Model A\" # Stop early\n",
    "    else:\n",
    "        # Escalate to expensive Model B\n",
    "        answer_b = model_b.generate(query)\n",
    "        return answer_b, \"Model B\" # Higher cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0681c825",
   "metadata": {},
   "source": [
    "## Coding \n",
    "**Train a basic router model. Using an open dataset like LMSYS Chatbot Arena results, extract features (possibly the user query text or embedding) and labels (which model among a pair won). Train a classifier (e.g. a small BERT or even logistic regression on embedding features) to predict if a cheaper model’s output will be rated as good as GPT-5’s. Then evaluate: for new queries, use the classifier’s prediction to decide routing (cheap vs. expensive). How much cost can you cut while maintaining quality above a threshold?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd53ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /Users/namitha/Library/Python/3.9/lib/python/site-packages (4.4.2)\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: filelock in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from datasets) (1.3.1)\n",
      "Requirement already satisfied: packaging in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: certifi in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.21.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence_transformers)\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from sentence_transformers) (2.8.0)\n",
      "Collecting scikit-learn (from sentence_transformers)\n",
      "  Downloading scikit_learn-1.6.1-cp39-cp39-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: scipy in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: Pillow in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from sentence_transformers) (11.3.0)\n",
      "Collecting huggingface-hub<2.0,>=0.25.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading regex-2025.11.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading tokenizers-0.22.2-cp39-abi3-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets) (2.6.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence_transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.15.0)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence_transformers)\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence_transformers)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/namitha/Library/Python/3.9/lib/python/site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.1.8)\n",
      "Downloading sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.2-cp39-abi3-macosx_11_0_arm64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.11.3-cp39-cp39-macosx_11_0_arm64.whl (288 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl (447 kB)\n",
      "Downloading scikit_learn-1.6.1-cp39-cp39-macosx_12_0_arm64.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, safetensors, regex, joblib, scikit-learn, huggingface-hub, tokenizers, transformers, sentence_transformers\n",
      "\u001b[2K  Attempting uninstall: huggingface-hub\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/9\u001b[0m [scikit-learn]\n",
      "\u001b[2K    Found existing installation: huggingface_hub 1.3.1━━━━━━━━\u001b[0m \u001b[32m4/9\u001b[0m [scikit-learn]\n",
      "\u001b[2K    Uninstalling huggingface_hub-1.3.1:m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/9\u001b[0m [scikit-learn]\n",
      "\u001b[2K      Successfully uninstalled huggingface_hub-1.3.1━━━━━━━━━━\u001b[0m \u001b[32m4/9\u001b[0m [scikit-learn]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9/9\u001b[0m [sentence_transformers]ence_transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed huggingface-hub-0.36.0 joblib-1.5.3 regex-2025.11.3 safetensors-0.7.0 scikit-learn-1.6.1 sentence_transformers-5.1.2 threadpoolctl-3.6.0 tokenizers-0.22.2 transformers-4.57.3\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets sentence_transformers scikit-learn matplotlib\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "login(\"huggingface_token\")\n",
    "dataset = load_dataset(\"lmsys/chatbot_arena_conversations\")\n",
    "\n",
    "import pandas as pd\n",
    "dataset = pd.DataFrame(dataset['train'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43a5730",
   "metadata": {},
   "source": [
    "**There are a total of 20 models used for preference comparision in LMSYS arena dataset. We can split them into cheap vs expensive based on the approximate costing.**\n",
    "\n",
    "\n",
    "Then train a binary classifier to output probability of strong model being chosen over weak. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "344d227b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fastchat-t5-3b', 'mpt-7b-chat', 'wizardlm-13b', 'chatglm-6b', 'oasst-pythia-12b', 'llama-13b', 'claude-instant-v1', 'vicuna-13b', 'alpaca-13b', 'gpt-4', 'vicuna-7b', 'koala-13b', 'gpt-3.5-turbo', 'stablelm-tuned-alpha-7b', 'RWKV-4-Raven-14B', 'guanaco-33b', 'dolly-v2-12b', 'gpt4all-13b-snoozy', 'palm-2', 'claude-v1']\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "columns_a = dataset['model_a']\n",
    "unique_columns_a = list(set(columns_a))\n",
    "print(unique_columns_a)\n",
    "columns_b = dataset['model_b']\n",
    "unique_columns_b = list(set(columns_b))\n",
    "print(len(unique_columns_b))\n",
    "\n",
    "# 1 maps to strong model, 0 maps to weak model\n",
    "tier_map = {\n",
    "    'gpt-4': 1, 'palm-2': 1, 'claude-v1': 1, 'gpt-3.5-turbo': 1, 'claude-instant-v1': 1,\n",
    "    'guanaco-33b': 0, 'llama-13b': 0, 'vicuna-13b': 0, 'vicuna-7b': 0, 'wizardlm-13b': 0,\n",
    "    'alpaca-13b': 0, 'koala-13b': 0, 'oasst-pythia-12b': 0, 'dolly-v2-12b': 0, 'mpt-7b-chat': 0,\n",
    "    'RWKV-4-Raven-14B': 0, 'gpt4all-13b-snoozy': 0, 'chatglm-6b': 0, 'fastchat-t5-3b': 0, \n",
    "    'stablelm-tuned-alpha-7b': 0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45438cef",
   "metadata": {},
   "source": [
    "**Map the models in comparision to 1 or 0 and filter out comparisions among same class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d04eb1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13621\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Map tiers to new columns\n",
    "dataset[\"model_a\"] = dataset[\"model_a\"].map(tier_map)\n",
    "dataset[\"model_b\"] = dataset[\"model_b\"].map(tier_map)\n",
    "\n",
    "# Apply filter\n",
    "filtered_dataset = dataset[\n",
    "    dataset[\"model_a\"].notna() &\n",
    "    dataset[\"model_b\"].notna() &\n",
    "    (dataset[\"model_a\"] != dataset[\"model_b\"])\n",
    "]\n",
    "\n",
    "print(len(filtered_dataset))\n",
    "print(filtered_dataset['model_a'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abd5bcf",
   "metadata": {},
   "source": [
    "**Extract and encode the query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2c59bafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13621\n"
     ]
    }
   ],
   "source": [
    "def extract_query(conversation):\n",
    "    \"\"\"\n",
    "    Extracts the content of the first 'user' role message \n",
    "    from a list of conversation turns.\n",
    "    \"\"\"\n",
    "    for turn in conversation:\n",
    "        if turn['role'] == 'user':\n",
    "            return turn['content']\n",
    "    return \"\"\n",
    "\n",
    "# Example Application on your Dataset\n",
    "# Assuming 'filtered_dataset' is your Hugging Face dataset object\n",
    "queries = [extract_query(filtered_dataset['conversation_a'].iloc[i]) for i in range(len(filtered_dataset))]\n",
    "\n",
    "print(len(queries))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8be220c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/huggingface_hub/utils/_http.py\", line 657, in hf_raise_for_status\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/httpx/_models.py\", line 829, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Client error '404 Not Found' for url 'https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/5z/2t6795n55wq1q_kmrdvlwflm0000gn/T/ipykernel_23322/1152804653.py\", line 8, in <module>\n",
      "    encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/sentence_transformers/SentenceTransformer.py\", line 327, in __init__\n",
      "    modules, self.module_kwargs = self._load_sbert_model(\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/sentence_transformers/SentenceTransformer.py\", line 2305, in _load_sbert_model\n",
      "    module = module_class.load(\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/sentence_transformers/models/Transformer.py\", line 365, in load\n",
      "    return cls(model_name_or_path=model_name_or_path, **init_kwargs)\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/sentence_transformers/models/Transformer.py\", line 102, in __init__\n",
      "    self.tokenizer = AutoTokenizer.from_pretrained(\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/transformers/models/auto/tokenization_auto.py\", line 1156, in from_pretrained\n",
      "    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py\", line 2039, in from_pretrained\n",
      "    for template in list_repo_templates(\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/transformers/utils/hub.py\", line 167, in list_repo_templates\n",
      "    return [\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/transformers/utils/hub.py\", line 167, in <listcomp>\n",
      "    return [\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/huggingface_hub/hf_api.py\", line 3297, in list_repo_tree\n",
      "    Example:\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/huggingface_hub/utils/_pagination.py\", line 37, in paginate\n",
      "    hf_raise_for_status(r)\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/huggingface_hub/utils/_http.py\", line 671, in hf_raise_for_status\n",
      "huggingface_hub.errors.RemoteEntryNotFoundError: 404 Client Error. (Request ID: Root=1-696779a4-106cc57410190c7126850210;4f14eb9f-d7a3-469f-ba3d-8804d7ef469a)\n",
      "\n",
      "Entry Not Found for url: https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false.\n",
      "additional_chat_templates does not exist on \"main\"\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/huggingface_hub/utils/_http.py\", line 657, in hf_raise_for_status\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/httpx/_models.py\", line 829, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Client error '404 Not Found' for url 'https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/5z/2t6795n55wq1q_kmrdvlwflm0000gn/T/ipykernel_23322/1152804653.py\", line 11, in <module>\n",
      "    encoder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/sentence_transformers/SentenceTransformer.py\", line 327, in __init__\n",
      "    modules, self.module_kwargs = self._load_sbert_model(\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/sentence_transformers/SentenceTransformer.py\", line 2305, in _load_sbert_model\n",
      "    module = module_class.load(\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/sentence_transformers/models/Transformer.py\", line 365, in load\n",
      "    return cls(model_name_or_path=model_name_or_path, **init_kwargs)\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/sentence_transformers/models/Transformer.py\", line 102, in __init__\n",
      "    self.tokenizer = AutoTokenizer.from_pretrained(\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/transformers/models/auto/tokenization_auto.py\", line 1156, in from_pretrained\n",
      "    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py\", line 2039, in from_pretrained\n",
      "    for template in list_repo_templates(\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/transformers/utils/hub.py\", line 167, in list_repo_templates\n",
      "    return [\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/transformers/utils/hub.py\", line 167, in <listcomp>\n",
      "    return [\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/huggingface_hub/hf_api.py\", line 3297, in list_repo_tree\n",
      "    Example:\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/huggingface_hub/utils/_pagination.py\", line 37, in paginate\n",
      "    hf_raise_for_status(r)\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/huggingface_hub/utils/_http.py\", line 671, in hf_raise_for_status\n",
      "huggingface_hub.errors.RemoteEntryNotFoundError: 404 Client Error. (Request ID: Root=1-696779a7-3aac4a8531d609244795c0ac;933c6307-c963-49b5-b3d5-7bfe9887f97d)\n",
      "\n",
      "Entry Not Found for url: https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false.\n",
      "additional_chat_templates does not exist on \"main\"\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/huggingface_hub/utils/_http.py\", line 657, in hf_raise_for_status\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/httpx/_models.py\", line 829, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Client error '404 Not Found' for url 'https://huggingface.co/api/models/sentence-transformers/paraphrase-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/5z/2t6795n55wq1q_kmrdvlwflm0000gn/T/ipykernel_23322/1152804653.py\", line 14, in <module>\n",
      "    encoder = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/sentence_transformers/SentenceTransformer.py\", line 327, in __init__\n",
      "    modules, self.module_kwargs = self._load_sbert_model(\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/sentence_transformers/SentenceTransformer.py\", line 2305, in _load_sbert_model\n",
      "    module = module_class.load(\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/sentence_transformers/models/Transformer.py\", line 365, in load\n",
      "    return cls(model_name_or_path=model_name_or_path, **init_kwargs)\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/sentence_transformers/models/Transformer.py\", line 102, in __init__\n",
      "    self.tokenizer = AutoTokenizer.from_pretrained(\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/transformers/models/auto/tokenization_auto.py\", line 1175, in from_pretrained\n",
      "    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py\", line 2039, in from_pretrained\n",
      "    for template in list_repo_templates(\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/transformers/utils/hub.py\", line 167, in list_repo_templates\n",
      "    return [\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/transformers/utils/hub.py\", line 167, in <listcomp>\n",
      "    return [\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/huggingface_hub/hf_api.py\", line 3297, in list_repo_tree\n",
      "    Example:\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/huggingface_hub/utils/_pagination.py\", line 37, in paginate\n",
      "    hf_raise_for_status(r)\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/huggingface_hub/utils/_http.py\", line 671, in hf_raise_for_status\n",
      "huggingface_hub.errors.RemoteEntryNotFoundError: 404 Client Error. (Request ID: Root=1-696779aa-3fd8a6ed1c1d9b4a326f8242;1de13376-c8b3-47b5-ac7b-a530cea4e857)\n",
      "\n",
      "Entry Not Found for url: https://huggingface.co/api/models/sentence-transformers/paraphrase-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false.\n",
      "additional_chat_templates does not exist on \"main\"\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/Users/namitha/Library/Python/3.9/lib/python/site-packages/executing/executing.py\", line 76, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "# Encode queries using embedding model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Use SentenceTransformer which has the encode() method\n",
    "# Try different model names if one fails\n",
    "try:\n",
    "    encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "except:\n",
    "    try:\n",
    "        encoder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    except:\n",
    "        # Fallback to paraphrase model\n",
    "        encoder = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def extract_features(prompts):\n",
    "    \"\"\"Extract features: embeddings + normalized length.\"\"\"\n",
    "    # SentenceTransformer.encode() returns numpy array\n",
    "    embeddings = encoder.encode(prompts, show_progress_bar=True, convert_to_numpy=True)\n",
    "    lengths = np.array([len(p) for p in prompts]).reshape(-1, 1)\n",
    "    norm_lengths = lengths / (lengths.max() + 1e-10)\n",
    "    return np.hstack((embeddings, norm_lengths))\n",
    "\n",
    "print(\"Encoding queries...\")\n",
    "X = extract_features(queries)\n",
    "print(f\"Feature shape: {X.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8dadeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels from winner column\n",
    "# Winner is either 'model_a' or 'model_b' (or None for tie)\n",
    "# Label = 1 if strong model (tier 1) won, 0 if weak model (tier 0) won\n",
    "\n",
    "def create_labels(filtered_dataset):\n",
    "    \"\"\"Create binary labels: 1 if strong model won, 0 if weak model won.\"\"\"\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(len(filtered_dataset)):\n",
    "        winner = filtered_dataset['winner'].iloc[i]\n",
    "        tier_a = filtered_dataset['model_a'].iloc[i]  # Already mapped to 0 or 1\n",
    "        tier_b = filtered_dataset['model_b'].iloc[i]  # Already mapped to 0 or 1\n",
    "        \n",
    "        if winner == 'model_a':\n",
    "            label = tier_a  # 1 if strong, 0 if weak\n",
    "        elif winner == 'model_b':\n",
    "            label = tier_b  # 1 if strong, 0 if weak\n",
    "        else:\n",
    "            # Tie or invalid - use None (will filter out)\n",
    "            label = None\n",
    "        \n",
    "        labels.append(label)\n",
    "    \n",
    "    return np.array(labels)\n",
    "\n",
    "y = create_labels(filtered_dataset)\n",
    "\n",
    "# Filter out None labels (ties)\n",
    "valid_mask = ~pd.isna(y)\n",
    "X_filtered = X[valid_mask]\n",
    "y_filtered = y[valid_mask].astype(int)\n",
    "queries_filtered = [q for i, q in enumerate(queries) if valid_mask[i]]\n",
    "\n",
    "print(f\"Valid examples: {len(y_filtered)}\")\n",
    "print(f\"Strong model wins (1): {np.sum(y_filtered == 1)} ({np.mean(y_filtered == 1)*100:.1f}%)\")\n",
    "print(f\"Weak model wins (0): {np.sum(y_filtered == 0)} ({np.mean(y_filtered == 0)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11739c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train binary classifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_filtered, y_filtered, test_size=0.2, random_state=42, stratify=y_filtered\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(X_train)} examples\")\n",
    "print(f\"Test set: {len(X_test)} examples\")\n",
    "\n",
    "# Train logistic regression classifier\n",
    "classifier = LogisticRegression(random_state=42, max_iter=1000)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_proba = classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Classifier Performance:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ROC-AUC: {auc:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Weak Model Wins', 'Strong Model Wins']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f60ba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve: Strong Model Wins Prediction')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Classifier ready for routing decisions!\")\n",
    "print(\"Use classifier.predict_proba(query_embedding)[0][1] to get probability of strong model winning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4191d233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Router function: use classifier to decide routing\n",
    "def router_policy(query, classifier, encoder, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Router policy using classifier.\n",
    "    Returns: ('strong' or 'weak', probability)\n",
    "    \"\"\"\n",
    "    # Encode query\n",
    "    query_features = extract_features([query])\n",
    "    \n",
    "    # Predict probability that strong model wins\n",
    "    proba_strong = classifier.predict_proba(query_features)[0][1]\n",
    "    \n",
    "    if proba_strong >= threshold:\n",
    "        return 'strong', proba_strong\n",
    "    else:\n",
    "        return 'weak', proba_strong\n",
    "\n",
    "# Example usage\n",
    "example_query = queries_filtered[0]\n",
    "route, prob = router_policy(example_query, classifier, encoder, threshold=0.5)\n",
    "print(f\"Example query: {example_query[:100]}...\")\n",
    "print(f\"Routing decision: {route} (probability={prob:.3f})\")\n",
    "print(f\"Expected cost: {'HIGH' if route == 'strong' else 'LOW'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a516a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost analysis: Evaluate cost savings at different thresholds\n",
    "# Assume: strong model = $0.01 per query, weak model = $0.001 per query\n",
    "\n",
    "COST_STRONG = 0.01\n",
    "COST_WEAK = 0.001\n",
    "\n",
    "thresholds = np.arange(0.3, 0.95, 0.05)\n",
    "results = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    # Route all test queries\n",
    "    routes = []\n",
    "    for query in queries_filtered[::10][:len(X_test)]:  # Sample for speed\n",
    "        route, _ = router_policy(query, classifier, encoder, threshold=thresh)\n",
    "        routes.append(route)\n",
    "    \n",
    "    # Calculate costs\n",
    "    n_strong = sum(1 for r in routes if r == 'strong')\n",
    "    n_weak = len(routes) - n_strong\n",
    "    total_cost = n_strong * COST_STRONG + n_weak * COST_WEAK\n",
    "    \n",
    "    # Compare to baseline (always use strong)\n",
    "    baseline_cost = len(routes) * COST_STRONG\n",
    "    cost_savings = (baseline_cost - total_cost) / baseline_cost * 100\n",
    "    \n",
    "    results.append({\n",
    "        'threshold': thresh,\n",
    "        'total_cost': total_cost,\n",
    "        'cost_savings_pct': cost_savings,\n",
    "        'strong_ratio': n_strong / len(routes)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Cost Analysis:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ce990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Cost savings vs threshold\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(results_df['threshold'], results_df['cost_savings_pct'], 'o-', linewidth=2)\n",
    "plt.xlabel('Routing Threshold')\n",
    "plt.ylabel('Cost Savings (%)')\n",
    "plt.title('Cost Savings vs Routing Threshold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(results_df['threshold'], results_df['strong_ratio'], 's-', linewidth=2, color='orange')\n",
    "plt.xlabel('Routing Threshold')\n",
    "plt.ylabel('Fraction Routed to Strong Model')\n",
    "plt.title('Strong Model Usage vs Threshold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"Max cost savings: {results_df['cost_savings_pct'].max():.1f}%\")\n",
    "print(f\"At threshold={results_df.loc[results_df['cost_savings_pct'].idxmax(), 'threshold']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2b68b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dfb06c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be71d8d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5cded7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff50e48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
