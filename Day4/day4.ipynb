{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98067fc9",
   "metadata": {},
   "source": [
    "## Design: \n",
    "Suppose our current system uses an $\\epsilon$-greedy policy for model selection (mostly choosing GPT-5, occasionally a cheaper model). Define a logging schema for each interaction: what context, action, and probability information must we log to enable unbiased OPE of a new routing policy? Outline the data structure clearly.\n",
    "\n",
    "Data to be logged : \n",
    "\n",
    "1. context xi\n",
    "2. action taken ai\n",
    "3. reward we got - ri\n",
    "4. probability info - probability of ai being taken given xi context along with epsilon.\n",
    "\n",
    "                    - if ai = gpt-5(best action) then pi = 1 - e + (e/k)\n",
    "\n",
    "                    - if ai != gpt-5 then pi = e/k\n",
    "                    \n",
    "                    where k is the number of actions, here 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad962626",
   "metadata": {},
   "source": [
    "## Coding: \n",
    "Implement IPS and Doubly Robust estimators for off-policy policy value. Use a synthetic logged dataset (e.g., generated by a known policy on a multi-armed bandit) and a candidate target policy. Compare their estimates to the ground-truth value. Then implement the SWITCH estimator: for each instance, decide to use IPS or a model prediction based on whether the importance weight is below a threshold. Show that SWITCH yields lower Mean Squared Error than plain IPS when the target policy is significantly different from the logging policy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ee919ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Expected Reward of Target Policy: 0.3\n",
      "IPS Estimated Reward: 0.3267\n",
      "DR Estimated Reward:  0.3379\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# 1. Setup\n",
    "# Mean rewards for GPT-5 (0.8) and Cheap (0.3)\n",
    "true_rewards = [0.8, 0.3] \n",
    "old_epsilon = 0.3\n",
    "num_actions = 1000 # Increased for better estimation\n",
    "\n",
    "actions = []\n",
    "rewards = []\n",
    "ps = []\n",
    "\n",
    "# 2. Logging Data (Logging Policy)\n",
    "for i in range(num_actions):\n",
    "    # Epsilon-greedy\n",
    "    if random.random() < old_epsilon:\n",
    "        action = random.choice(range(len(true_rewards)))\n",
    "    else:\n",
    "        action = true_rewards.index(max(true_rewards))\n",
    "    \n",
    "    actions.append(action)\n",
    "    reward = np.random.binomial(1, p=true_rewards[action])\n",
    "    rewards.append(reward)\n",
    "\n",
    "    # Correct Propensity Calculation\n",
    "    if action == 0: # GPT-5 was the 'best' in logging policy\n",
    "        p = (1 - old_epsilon) + (old_epsilon / len(true_rewards))\n",
    "    else:\n",
    "        p = old_epsilon / len(true_rewards)\n",
    "    ps.append(p)\n",
    "\n",
    "# 3. Direct Method Estimation (q-model)\n",
    "# We estimate the expected reward for each action based on historical data\n",
    "q_model = [0.0, 0.0]\n",
    "for a in [0, 1]:\n",
    "    # Mean reward = sum of rewards for action a / count of action a\n",
    "    mask = [actions[i] == a for i in range(num_actions)]\n",
    "    if sum(mask) > 0:\n",
    "        q_model[a] = np.mean([rewards[i] for i in range(num_actions) if actions[i] == a])\n",
    "\n",
    "# 4. Target Policy Definition\n",
    "# Suppose the new policy ALWAYS picks the Cheap model (index 1)\n",
    "def target_policy_prob(action_idx):\n",
    "    return 1.0 if action_idx == 1 else 0.0\n",
    "\n",
    "# 5. Evaluation (IPS and DR)\n",
    "ips_sum = 0 \n",
    "dr_sum = 0\n",
    "\n",
    "for i in range(num_actions):\n",
    "    # Probability target policy would take the action that WAS taken\n",
    "    pi_a = target_policy_prob(actions[i])\n",
    "    \n",
    "    # Probability target policy would take its PREFERRED action (for DR baseline)\n",
    "    # Since it's deterministic, we know it's action 1\n",
    "    target_action = 1 \n",
    "    \n",
    "    # IPS Calculation\n",
    "    ips_sum += rewards[i] * (pi_a / ps[i])\n",
    "\n",
    "    # DR Calculation\n",
    "    # Term 1: The model's guess for the target action\n",
    "    # Term 2: The error correction (only active if logging action == target action)\n",
    "    error_correction = (pi_a / ps[i]) * (rewards[i] - q_model[actions[i]])\n",
    "    dr_sum += q_model[target_action] + error_correction\n",
    "\n",
    "print(f\"True Expected Reward of Target Policy: {true_rewards[1]}\")\n",
    "print(f\"IPS Estimated Reward: {ips_sum / num_actions:.4f}\")\n",
    "print(f\"DR Estimated Reward:  {dr_sum / num_actions:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d8df5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switch Estimated Reward (tau=5.0): 0.3379\n"
     ]
    }
   ],
   "source": [
    "# Constants for the Switch\n",
    "# If 1/p is greater than this, we stop trusting the data and use the model only\n",
    "tau = 5.0 \n",
    "\n",
    "switch_sum = 0\n",
    "\n",
    "for i in range(num_actions):\n",
    "    # 1. Target policy info\n",
    "    target_action = 1  # Our new policy always wants the Cheap Model\n",
    "    pi_a = 1.0 if actions[i] == target_action else 0.0\n",
    "    \n",
    "    baseline_q = q_model[target_action]\n",
    "    \n",
    "    weight = pi_a / ps[i]\n",
    "    \n",
    "    if weight <= tau:\n",
    "        correction = weight * (rewards[i] - q_model[actions[i]])\n",
    "        switch_sum += baseline_q + correction\n",
    "    else:\n",
    "        switch_sum += baseline_q\n",
    "\n",
    "final_switch_reward = switch_sum / num_actions\n",
    "print(f\"Switch Estimated Reward (tau={tau}): {final_switch_reward:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
